{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8f17c26ef5290d45b564a2185541d945",
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Osnabrück University - Machine Learning (Summer Term 2018) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0363ecd47553f55d98a7188069b1912a",
     "grade": false,
     "grade_id": "h01",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dbe742d32e759824b79c02a9acdf8ab3",
     "grade": false,
     "grade_id": "h02",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, June 3, 2018**. If you need help (and Google and other resources were not enough), feel free to contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a4a7a28fc70bb1656f3eb43a4061ec28",
     "grade": false,
     "grade_id": "cell-78e418c8c7c6b9cb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 0: Math recap (Conditional Probability) [2 Bonus Points]\n",
    "\n",
    "This exercise is supposed to be very easy and is voluntary. There will be a similar exercise on every sheet. It is intended to revise some basic mathematical notions that are assumed throughout this class and to allow you to check if you are comfortable with them. Usually you should have no problem to answer these questions offhand, but if you feel unsure, this is a good time to look them up again. You are always welcome to discuss questions with the tutors or in the practice session. Also, if you have a (math) topic you would like to recap, please let us know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "855316bdf01ce31dab06b89382d0c83f",
     "grade": false,
     "grade_id": "math-cprob-q1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**a)** Explain the idea of conditional probability. How is it defined?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "9a9f9702710b581756a95952bd3c8336",
     "grade": true,
     "grade_id": "math-cprob-a1",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "*Conditional probability describes the measure of an event given that another event already occured.*\n",
    "\n",
    "*Notation:* \n",
    "- probability of event A = P(A)\n",
    "- probability of event B = P(B)\n",
    "- probability of event A, given B = P(A|B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "46198461fba50cb38ba7278aca6b935c",
     "grade": false,
     "grade_id": "math-cprob-q2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**b)** What is Bayes' theorem? What are its applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "fe506eb4d056aad3a69041b3e97ee42c",
     "grade": true,
     "grade_id": "math-cprob-a2",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "*Bayes' theorem describes the probability of an event given prior knowledge or beliefs about conditions that might be related to an event, hence it is possible to update predicitions given new evidence. Therefore it can be used to calculate conditional probability.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7b1e731d96f98286b8a7c10c0ce192ee",
     "grade": false,
     "grade_id": "math-cprob-q3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**c)** What does the law of total probability state? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "21400076933e23233fb58f7297b85cbb",
     "grade": true,
     "grade_id": "math-cprob-a3",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "*The law of total probability breaks up probability calculations into distinct parts and is used to find the probability of an event (the total probability), based on a related event when not enough is known about the particular event's probabilities to calculate it directly.*\n",
    "\n",
    "*It relates marginal probabilities to conditional probabilities and is the basis for Bayes' rule.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6c4f5c0c44b0d7db516c6e967a66e6b7",
     "grade": false,
     "grade_id": "ex1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 1: Multilayer Perceptron (MLP) [10 Points]\n",
    "\n",
    "Last week you implemented a simple perceptron. We discussed that one can use multiple perceptrons to build a network. This week you will build your own MLP. Again the following code cells are just a guideline. If you feel like it, just follow the algorithm steps and implement the MLP yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8c4335c4a1f1b67681cb483fdd3793d0",
     "grade": false,
     "grade_id": "ex1a_intro",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Implementation\n",
    "\n",
    "In the following you will be guided through implementing an MLP step by step. Instead of sticking to this guide, you are free to take a complete custom approach instead if you wish.\n",
    "\n",
    "We will take a bottom-up approach: Starting from an individual **perceptron** (aka neuron), we will derive a **layer of perceptrons** and end up with a **multilayer perceptron** (aka neural network). Each step will be implemented as its own python *class*. Such a class defines a type of element which can be instantiated multiple times. You can think of the relation between such instances and their designated classes as individuals of a specific population (e.g. Bernard and Bianca are both individuals of the population mice). Class definitions contain methods, which can be used to manipulate instance of that class or to make it perform specific actions — again, taking the population reference, each mouse of the mice population would for example have the method `eat_cheese()`.\n",
    "\n",
    "To guide you along, all required classes and functions are outlined in valid python code with extensive comments. You just need to fill in the gaps. For each method the [docstring](https://www.python.org/dev/peps/pep-0257/#what-is-a-docstring) (the big comment contained by triple quotes at the beginning of the method) describes the arguments that specific method accepts (`Args`) and the values it is expected to return (`Returns`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e76fef5be13db582fe1dbbe08b17d52e",
     "grade": false,
     "grade_id": "ex1a_intro2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Perceptron\n",
    "Similar to last week you here need to implement a perceptron. But instead of directly applying it, we will define a class which is reusable to instantiate a theoretically infinite amount of individual perceptrons. We will need the following three functionalities:\n",
    "\n",
    "#### Weight initialization\n",
    "\n",
    "The weights are initialized by sampling values from a standard normal distribution. There are as many weights as there are values in the input vector and an additional one for the perceptron's bias.\n",
    "\n",
    "#### Forward-Propagation / Activation\n",
    "\n",
    "Calculate the weighted sums of a neuron's inputs and apply it's activation function $\\sigma$. The output vector $o$ of perceptron $j$ of layer $k$ given an input $x$ (the output of the previous layer) in a neural network is given by the following formula. Note: $N$ gives the number of values of a given vector, $w_{j,0}(k)$ specifies the bias of perceptron $j$ in layer $k$ and $w_{j,1...N(x)}(k)$ the other weights of perceptron $j$ in layer $k$.\n",
    "\n",
    "$$o_{k,j}(x) = \\sigma\\left(w_{j,0}(k)+\\sum\\limits_{i=1}^{N(x)} x_i w_{j,i}(k)\\right)$$\n",
    "\n",
    "Think of the weights $w(k)$ as a matrix being located in-between layer $k$ and the layer located *to its left* in the network. So values flowing from layer $k-1$ to layer $k$ are weighted by the values of $w(k)$. As activation function we will use the sigmoid function because of its nice derivative (needed later):\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\sigma(x) &= \\frac{1}{1 + \\exp{(-x)}}\\\\\n",
    "\\frac{d\\sigma}{dx}(x) &= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "\\end{align*}$$\n",
    "\n",
    "#### Back-Propagation / Adaptation\n",
    "In order to learn something the perceptron needs to slowly adjust its weights. Each weight $w_{j,i}$ in layer $k$ is adjusted by a value $\\Delta w_{j,i}$ given a learning rate $\\epsilon$, the previous layer's output (or, for the first hidden layer, the network's input) $o_{k-1,i}(x)$ and the layer's error signals $\\delta(k)$ (which will be calculated by the MultilayerPerceptron):\n",
    "\n",
    "$$\\Delta w_{j,i}(k) = \\epsilon\\, \\delta_j(k) o_{k-1,i}(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dda19fefaff078a764a3440a12de2532",
     "grade": true,
     "grade_id": "ex1a_solution",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation function σ.\n",
    "# We use scipy's builtin because it fixes some NaN problems for us.\n",
    "# sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"Single neuron handling its own weights and bias.\"\"\"\n",
    "\n",
    "    def __init__(self, dim_in, act_func=sigmoid):\n",
    "        \"\"\"Initialize a new neuron with its weights and bias.\n",
    "\n",
    "        Args:\n",
    "            dim_in (int): Dimensionality of the data coming into this perceptron. \n",
    "                In a network of perceptrons this basically represents the \n",
    "                number of neurons in the layer before this neuron's layer. \n",
    "                Used for generating the perceptron's weights vector, which \n",
    "                not only includes one weight per input but also an additional \n",
    "                bias weight.\n",
    "            act_fun (function): Function to apply on activation.\n",
    "        \"\"\"\n",
    "        self.act_func = act_func\n",
    "        # Set self.weights\n",
    "        # generate random starting weights (number of input dimensions + 1 for bias) \n",
    "        self.weights= np.random.rand(dim_in+1)\n",
    "\n",
    "    def activate(self, x):\n",
    "        \"\"\"Activate this neuron with a specific input.\n",
    "\n",
    "        Calculate the weighted sum of inputs and apply the activation function.\n",
    "\n",
    "        Args:\n",
    "            x (ndarray): Vector of input values.\n",
    "\n",
    "        Returns:\n",
    "            float: A real number representing the perceptron's activation after \n",
    "            calculating the weighted sum of inputs and applying the \n",
    "            perceptron's activation function.\n",
    "        \"\"\"\n",
    "        # Return the activation value\n",
    "        # use activation function on weighted sum of inputs (first column of data should be 1)\n",
    "        \n",
    "        return self.act_func(self.weights @ np.append(1, x))\n",
    "\n",
    "    def adapt(self, x, delta, rate=0.03):\n",
    "        \"\"\"Adapt this neuron's weights by a specific delta.\n",
    "\n",
    "        Args:\n",
    "            x (ndarray): Vector of input values.\n",
    "            delta (float): Weight adaptation delta value.\n",
    "            rate (float): Learning rate.\n",
    "        \"\"\"\n",
    "        # Adapt self.weights according to the update rule\n",
    "        # adjust weights (again make sure that first column is 1 for bias)\n",
    "        self.weights += rate * delta * np.append(1, x)\n",
    "\n",
    "\n",
    "_p = Perceptron(2)\n",
    "assert _p.weights.size == 3, \"Should have a weight per input and a bias.\"\n",
    "assert isinstance(_p.activate([2, 1]), float), \"Should activate as scalar.\"\n",
    "assert -1 <= _p.activate([100, 100]) <= 1, \"Should activate using sigmoid.\"\n",
    "_p.weights = np.array([.5, .5, .5])\n",
    "_p.adapt(np.array([2, 3]), np.array(.5))\n",
    "assert np.allclose(_p.weights, [0.515, 0.53, 0.545]), \\\n",
    "        \"Should update weights correctly.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e92e6a95a27e6702da7e4eeeff1ea574",
     "grade": false,
     "grade_id": "ex1b_intro",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### PerceptronLayer\n",
    "A `PerceptronLayer` is a combination of multiple `Perceptron` instances. It mainly is concerened with passing input and delta values to its individual neurons. There is no math to be done here!\n",
    "\n",
    "#### Initialization\n",
    "\n",
    "When initializing a `PerceptronLayer` (like this: `layer = PerceptronLayer(5, 3)`), the `__init__` function is called. It creates a list of `Perceptron`s: For each output value there must be one perceptron. Each of those perceptrons receives the same inputs and the same activation function as the perceptron layer.\n",
    "\n",
    "#### Activation\n",
    "\n",
    "During the activation step, the perceptron layer activates each of its perceptrons. These values will not only be needed for forward propagation but will also be needed for implementing backpropagation in the `MultilayerPerceptron` (coming up next).\n",
    "\n",
    "#### Adaptation\n",
    "\n",
    "To update its perceptrons, the perceptron layer adapts each one with the corresponding delta. For this purpose, the MLP passes a list of input values and a list of deltas to the adaptation function. The inputs are passed to *all* perceptrons. The list of deltas is exactly as long as the list of perceptrons: The first delta is for the first perceptron, the second for the second, etc. The delta values themselves will be computed by the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "76d76b823a657e790bdd16544db0a6cc",
     "grade": true,
     "grade_id": "ex1b_solution",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class PerceptronLayer:\n",
    "    \"\"\"Layer of multiple neurons.\n",
    "    \n",
    "    Attributes:\n",
    "        perceptrons (list): List of perceptron instances in the layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in, dim_out, act_func=sigmoid):\n",
    "        \"\"\"Initialize the layer as a list of individual neurons.\n",
    "\n",
    "        A layer contains as many neurons as it has outputs, each\n",
    "        neuron has as many input weights (+ bias) as the layer has inputs.\n",
    "\n",
    "        Args:\n",
    "            dim_in (int): Dimensionality of the expected input values,\n",
    "                also the size of the previous layer of a neural network.\n",
    "            dim_out (int): Dimensionality of the output, also the requested \n",
    "                amount of in this layer and the input dimension of the\n",
    "                next layer.\n",
    "            act_func (function): Activation function to use in each perceptron of\n",
    "                this layer.\n",
    "        \"\"\"\n",
    "        # Set self.perceptrons to a list of Perceptrons\n",
    "        # create a list if perceptrons\n",
    "        self.perceptrons = [Perceptron(dim_in, act_func) for dim in range(dim_out)]\n",
    "\n",
    "    def activate(self, x):\n",
    "        \"\"\"Activate this layer by activating each individual neuron.\n",
    "\n",
    "        Args:\n",
    "            x (ndarray): Vector of input values.\n",
    "\n",
    "        Retuns:\n",
    "            ndarray: Vector of output values which can be \n",
    "            used as input to another PerceptronLayer instance.\n",
    "        \"\"\"\n",
    "        # return the vector of activation values\n",
    "        # return as ndarray\n",
    "        return np.array([percep.activate(x) for percep in self.perceptrons])\n",
    "\n",
    "    def adapt(self, x, deltas, rate=0.03):\n",
    "        \"\"\"Adapt this layer by adapting each individual neuron.\n",
    "\n",
    "        Args:\n",
    "            x (ndarray): Vector of input values.\n",
    "            deltas (ndarray): Vector of delta values.\n",
    "            rate (float): Learning rate.\n",
    "        \"\"\"\n",
    "        # Update all the perceptrons in this layer (using corresponding delta)\n",
    "        for percep, delta in zip(self.perceptrons, deltas):\n",
    "            percep.adapt(x, delta, rate)\n",
    "        \n",
    "    @property\n",
    "    def weight_matrix(self):\n",
    "        \"\"\"Helper property for getting this layer's weight matrix.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: All the weights for this perceptron layer.\n",
    "        \"\"\"\n",
    "        return np.asarray([p.weights for p in self.perceptrons]).T\n",
    "\n",
    "\n",
    "_l = PerceptronLayer(3, 2)\n",
    "assert len(_l.perceptrons) == 2, \"Should have as many perceptrons as outputs.\"\n",
    "assert len(_l.activate([1,2,3])) == 2, \"Should provide correct output amount.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2c6591fb0712e9991fdea1ea6e83cb33",
     "grade": false,
     "grade_id": "ex1c_intro",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### MultilayerPerceptron\n",
    "\n",
    "#### Forward-Propagation / Activation\n",
    "Propagate the input value $x$ through each layer of the network, employing the output of the previous layer as input to the next layer.\n",
    "\n",
    "#### Back-Propagation / Adaptation\n",
    "This is the most complex step of the whole task. Split into three separate parts:\n",
    "\n",
    "1. ***Forward propagation***: Compute the outputs for each individual layer – similar to the forward-propagation step above, but we need to keep track of the intermediate results to compute each layer's errors. That means: Store the input as the first \"output\" and then activate each of the network's layers using the *previous* layer's output and store the layer's activation result.\n",
    "\n",
    "2. ***Backward propagation***: Calculate each layer's error signals $\\delta_i(k)$. The important part here is to do so from the last to the first array, because each layer's error depends on the error from its following layer. Note: The first part of this formula makes use of the activation functions derivative $\\frac{d\\sigma}{dx}(k)$.\n",
    "\n",
    "    $$\\delta_i(k) = o_i(k)\\ (1 - o_i(k))\\ \\sum\\limits_{j=1}^{N(k+1)} w_{ji}(k+1,k)\\delta_j(k+1)$$\n",
    "\n",
    "    (*Hint*: For the last layer (i.e. the first you calculate the $\\delta$ for) the sum in the formula above is the total network error. For all preceding layers $k$ you need to recalculate `e` using the $\\delta$ and weights of layer $k+1$. We already implemented a help function for you to access the weights of a specific layer. Check the `PerceptronLayer` if you did not find it yet.)\n",
    "\n",
    "3. ***Adaptation***: Call each layers adaptation function with its input, its designated error signals and the given learning rate.\n",
    "\n",
    "Hint: The last two steps can be performed in a single loop if you wish, but make sure to use the non-updated weights for the calculation of the next layer's error signals!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3b9a6a3f957dbcd462792a260a6ea9aa",
     "grade": true,
     "grade_id": "ex1c_solution",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class MultilayerPerceptron:\n",
    "    \"\"\"Network of perceptrons, also a set of multiple perceptron layers.\n",
    "    \n",
    "    Attributes:\n",
    "        layers (list): List of perceptron layers in the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, *layers):\n",
    "        \"\"\"Initialize a new network, madeup of individual PerceptronLayers.\n",
    "\n",
    "        Args:\n",
    "            *layers: Arbitrarily many PerceptronLayer instances.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "\n",
    "    def activate(self, x):\n",
    "        \"\"\"Activate network and return the last layer's output.\n",
    "\n",
    "        Args:\n",
    "            x (ndarray): Vector of input values.\n",
    "\n",
    "        Returns:\n",
    "            (ndarray): Vector of output values from the last layer of the \n",
    "            network after propagating forward through the network.\n",
    "        \"\"\"\n",
    "        # Propagate activation through the network\n",
    "        # and return output for last layer\n",
    "        for layer in self.layers:\n",
    "            x = layer.activate(x)\n",
    "        #return output from last layer\n",
    "        return x\n",
    "\n",
    "    def adapt(self, x, t, rate=0.03):\n",
    "        \"\"\"Adapt the whole network given an input and expected output.\n",
    "\n",
    "        Args:\n",
    "            x (ndarray): Vector of input values.\n",
    "            t (ndarray): Vector of target values (expected outputs).\n",
    "            rate (float): Learning rate.\n",
    "        \"\"\"\n",
    "        # Activate each layer and collect intermediate outputs.\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Calculate error 'e' between t and network output.\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # Backpropagate error through the network computing\n",
    "        # intermediate delta and adapting each layer.\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "152a3c61e96dc5165054dea4632beeec",
     "grade": false,
     "grade_id": "ex1d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ea58b34094bb368ee10234116e5d381f",
     "grade": false,
     "grade_id": "ex1d_intro",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Problem Definition\n",
    "Before we start, we need a problem to solve. In the following cell we first generate some three dimensional data (= $\\text{input_dim}$) between 0 and 1 and label all data according to a binary classification: If the data is close to the center (radius < 2.5), it belongs to one class, if it is further away from the center it belongs to the other class.\n",
    "\n",
    "In the cell below we visualize the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform(a, b, n=1):\n",
    "    \"\"\"Returns n floats uniformly distributed between a and b.\"\"\"\n",
    "    return (b - a) * np.random.random_sample(n) + a\n",
    "\n",
    "\n",
    "n = 1000\n",
    "radius = 5\n",
    "r = np.append(uniform(0, radius * .5, n // 2),\n",
    "              uniform(radius * .7, radius, n // 2))\n",
    "angle = uniform(0, 2 * np.pi, n)\n",
    "x = r * np.sin(angle) + uniform(-radius, radius, n)\n",
    "y = r * np.cos(angle) + uniform(-radius, radius, n)\n",
    "inputs = np.vstack((x, y)).T\n",
    "targets = np.less(np.linalg.norm(inputs, axis=1), radius * .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dfb39443f15598255ec807893a50f1b4",
     "grade": false,
     "grade_id": "ex1d_demo1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure('Data')\n",
    "plt.suptitle('Labeled Data')\n",
    "plt.scatter(*inputs.T, 2, c=targets, cmap='RdYlBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c6f21cca2778921b273204b8a4d84e2f",
     "grade": false,
     "grade_id": "ex1d_intro2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Model Design\n",
    "The following cell already contains a simple model with a single layer. Play around with some different configurations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dfaeffae61bc519a129b8ab7becd5e02",
     "grade": true,
     "grade_id": "ex1d_solution1",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "MLP = MultilayerPerceptron(\n",
    "    PerceptronLayer(2, 1),\n",
    ")\n",
    "# Adapt this MLP\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5a6f6361b876ec8e0d7ea4317c04e23e",
     "grade": false,
     "grade_id": "ex1d_intro3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Training\n",
    "Train the network on random samples from the data. Try adjusting the epochs and watch the training performance closely using different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a70e152ec9ec1c0904af7e2d78a39ccd",
     "grade": false,
     "grade_id": "ex1d_demo2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from matplotlib import cm\n",
    "\n",
    "EPOCHS = 200000\n",
    "\n",
    "max_accuracy = 0\n",
    "\n",
    "plt.figure('Training')\n",
    "scatter = plt.scatter(*inputs.T, 2)\n",
    "plt.show()\n",
    "\n",
    "for i in range(1, EPOCHS + 1):\n",
    "    s = np.random.randint(0, len(targets))\n",
    "    MLP.adapt(inputs[s], targets[s])\n",
    "\n",
    "    if i % 2500 == 0:\n",
    "        outputs = np.squeeze([MLP.activate(x) for x in inputs])\n",
    "        predictions = np.round(outputs)\n",
    "        accuracy = np.sum(predictions == targets) / len(targets) * 100\n",
    "        if accuracy > max_accuracy:\n",
    "            max_accuracy = accuracy\n",
    "        scatter.set_color(cm.RdYlBu(outputs))\n",
    "        plt.title('Training {:.0f}%: {:.2f}%. Best accuracy: {:.2f}%'\n",
    "                  .format(i / EPOCHS * 100, accuracy, max_accuracy))\n",
    "        scatter.figure.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "294283d3bfe026b36a873684a5a202ee",
     "grade": false,
     "grade_id": "ex1d_intro4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1b95d49d3838ade7fee5c63b5e9dc6f7",
     "grade": false,
     "grade_id": "ex1d_demo3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure('Evaluation')\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "ax.scatter(*inputs.T, 2, c=outputs, cmap='RdYlBu')\n",
    "ax.set_title('Continuous Classification')\n",
    "ax = plt.subplot(2, 2, 2)\n",
    "ax.set_title('Discretized Classification')\n",
    "ax.scatter(*inputs.T, 2, c=np.round(outputs), cmap='RdYlBu')\n",
    "ax = plt.subplot(2, 2, 3)\n",
    "ax.set_title('Original Labels')\n",
    "ax.scatter(*inputs.T, 2, c=targets, cmap='RdYlBu')\n",
    "ax = plt.subplot(2, 2, 4)\n",
    "ax.set_title('Wrong Classifications')\n",
    "ax.scatter(*inputs.T, 2, c=(targets != np.round(outputs)), cmap='OrRd')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d9177bc80f95eab96110cf43135ac143",
     "grade": false,
     "grade_id": "ex1d_intro5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Results\n",
    "Document your results in the following cell. We are interested in which network configurations you tried and what accuracies they resulted in. Did you run into problems during training? Was it steady or did it get stuck? Did you recognize anything about the training process? How could we get better results? Tell us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9c0b30174d983f56bb737c2d54e2bfa0",
     "grade": true,
     "grade_id": "ex1d_solution",
     "locked": false,
     "points": 4,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "52dd1acb7e295a080086dbf1522163ab",
     "grade": false,
     "grade_id": "ex2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 2: MLP and RBFN [10 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "89c37df6a4896a21d1977f79290e3d91",
     "grade": false,
     "grade_id": "ex2_intro",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "This exercise is aimed at deepening the understanding of Radial Basis Function Networks and how they relate to Multilayer Perceptrons. Not all of the answers can be found directly in the slides - so when answering the (more algorithmic) questions, first take a minute and think about how you would go about solving them and if nothing comes to mind search the internet for a little bit. If you are interested in a real life application of both algorithms and how they compare take a look at this paper: [Comparison between Multi-Layer Perceptron and Radial Basis Function Networks for Sediment Load Estimation in a Tropical Watershed](http://file.scirp.org/pdf/JWARP20121000014_80441700.pdf)\n",
    "\n",
    "![Schematic of a RBFN](RBFN.png)\n",
    "\n",
    "We have prepared a little example that shows how radial basis function approximation works in Python. This is not an example implementation of a RBFN but illustrates the work of the hidden neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "from numpy.random import uniform\n",
    "\n",
    "from scipy.interpolate import Rbf\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "def func(x, y):\n",
    "    '''\n",
    "    This is the example function that should be fitted.\n",
    "    Its shape could be described as two peaks close to\n",
    "    each other - one going up, the other going down\n",
    "    '''\n",
    "    return (x + y) * np.exp(-4.0 * (x**2 + y**2))\n",
    "\n",
    "\n",
    "# number of training points (you may try different values here)\n",
    "training_size = 50\n",
    "\n",
    "# sample 'training_size' data points from the input space [-1,1]x[-1,1] ...\n",
    "x = uniform(-1.0, 1.0, size=training_size)\n",
    "y = uniform(-1.0, 1.0, size=training_size)\n",
    "\n",
    "# ... and compute function values for them.\n",
    "fvals = func(x, y)\n",
    "\n",
    "# get the aprroximation via RBF\n",
    "new_func = Rbf(x, y, fvals)\n",
    "\n",
    "\n",
    "# Plot both functions:\n",
    "# create a 100x100 grid of input values\n",
    "x_grid, y_grid = np.mgrid[-1:1:100j, -1:1:100j]\n",
    "\n",
    "plt.figure(\"Original Function\")\n",
    "# This plot represents the original function\n",
    "f_orig = func(x_grid, y_grid)\n",
    "plt.imshow(f_orig, extent=[-1, 1, -1, 1], cmap=plt.cm.jet)\n",
    "\n",
    "plt.figure(\"RBF Result\")\n",
    "# This plots the approximation of the original function by the RBF\n",
    "# if the plot looks strange try to run it again, the sampling\n",
    "# in the beginning is random\n",
    "f_new = new_func(x_grid, y_grid)\n",
    "plt.imshow(f_new, extent=[-1, 1, -1, 1], cmap=plt.cm.jet)\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)\n",
    "# scatter the datapoints that have been used by the RBF\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "53e6c796f0502901f1f111fda4023d24",
     "grade": false,
     "grade_id": "ex2_intro2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Radial Basis Function Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "843eb7ad4113f75a0c6f0dbc965e11ec",
     "grade": false,
     "grade_id": "ex2a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### What are radial basis functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5689b524ee20527f70ef0afcbda498ad",
     "grade": true,
     "grade_id": "ex2a_solution",
     "locked": false,
     "points": 1.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "db3f6f0431b118780083b72bfd3f0008",
     "grade": false,
     "grade_id": "ex2b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### What is the structure of a RBFN? You may also use the notion from the above included picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a39a5aabd1ef812d743c62f53783aaad",
     "grade": true,
     "grade_id": "ex2b_solution",
     "locked": false,
     "points": 1.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "616136fc39234c7a2471b4fca69a8983",
     "grade": false,
     "grade_id": "ex2c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### How is a RBFN trained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b34cadc5a56e11dbe7a7b7afbd20c7b9",
     "grade": true,
     "grade_id": "ex2c_solution",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "48193a09eb2de5d7346e4d4c4df375b2",
     "grade": false,
     "grade_id": "ex2_intro3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Comparison to the Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bfd0472e7a522f91f5ec6b44ab73f5de",
     "grade": false,
     "grade_id": "ex2d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### What do both models have in common? Where do they differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a3ebb0013c85f99414937e2faf60593f",
     "grade": true,
     "grade_id": "ex2d_solution",
     "locked": false,
     "points": 1.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7653a1317cbde9eeb5f7c5bb69e0f916",
     "grade": false,
     "grade_id": "ex2e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### How can classification in both networks be visualized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "73186d27f52007f372b4690524d09f8c",
     "grade": true,
     "grade_id": "ex2e_solution",
     "locked": false,
     "points": 1.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4492d9d9b4f66662527046a11aa7db60",
     "grade": false,
     "grade_id": "ex2f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### When would you use a RBFN instead of a Multilayer Perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "99d0193dbc6423f677b21156586a3802",
     "grade": true,
     "grade_id": "ex2f_solution",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
