{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "60019695a75826e426e31bb0a36ae88d",
     "grade": false,
     "grade_id": "cell-a26d5efc968151ce",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Osnabrück University - Machine Learning (Summer Term 2017) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9e81575aaa58c46d7d86ae35d368cdd4",
     "grade": false,
     "grade_id": "cell-932f957b5e17c264",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 02: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "edd3b1762dfdbc1780d925dd0144969f",
     "grade": false,
     "grade_id": "cell-a85c8970c448755a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "By now everyone should have found a group. If someone still has none but wants to participate in the course please contact one of the tutors.\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, April 22, 2018**. If you need help (and Google and other resources were not enough), feel free to contact your groups designated tutor or whom ever of us you run into first. Please upload your results to your group's studip folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5dfa7e093820395b90cff4f77942ac78",
     "grade": false,
     "grade_id": "math-euclid",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 0: Math recap (Euclidean Space) [2 Bonus Points]\n",
    "\n",
    "This exercise is supposed to be very easy and is voluntary. There will be a similar exercise on every sheet.\n",
    "It is intended to revise some basic mathematical notions that are assumed throughout this class and to allow you to check if you are comfortable with them.\n",
    "Usually you should have no problem to answer these questions offhand, but if you feel unsure, this is a good time to look them up again. You are always welcome to discuss questions with the tutors or in the practice session.\n",
    "Also, if you have a (math) topic you would like to recap, please let us know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "899ac70855dbd336b6edef96c4c1a6f5",
     "grade": false,
     "grade_id": "math-euclid-q1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**a)** What is a *Euclidean space*? What is the *Cartesian plane*? How are they usually denoted? How to write points in these spaces?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "10da800d32483f0d4bfa9cf43624b3c0",
     "grade": true,
     "grade_id": "math-euclid-a1",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "Euclidean n-space is n-dimensional space specified with the axioms and postulates of Euclidean geometry.\n",
    "\n",
    "Cartesian plane is a coordinate system which specifies points by a pair of numerical coordinates which are the signed distances to the point from two fixed perpendicular directed lines, measured in the same unit of length.\n",
    "\n",
    "Real number n-dimentional space is commonly denoted by $\\mathbb{R}^n$. \n",
    "\n",
    "The points are written as n-tuples of real numbers, e.g. ( $x_1, x_2, x_3, ...$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0bf071a4c6d1ff977bcd336eb96d7daa",
     "grade": false,
     "grade_id": "math-euclid-q2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**b)** What is the *norm* of a vector in a Euclidean space? How to *add* and *substract* two vectors? How is the *Euclidean distance* defined? Are there other ways to measure distances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "4f95335b2228d0766f6bdbbba83f857d",
     "grade": true,
     "grade_id": "math-euclid-a2",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "The Euclidean norm assigns to each vector the length of its arrow, it is also called the magnitude of a vector.\n",
    "\n",
    "Vectors can be added by the so-called parallelogram method. For two vectors $A$ and $B$, the vector sum $A+B$ is obtained by placing them head to tail and drawing the vector from the free tail to the free head. In Cartesian coordinates, vector addition can be performed simply by adding the corresponding components of the vectors, so if $A=(a_1,a_2,...,a_n)$ and $B=(b_1,b_2,...,b_n)$. \n",
    "\n",
    "A vector difference is equivalent to a vector sum with the orientation of the second vector reversed, i.e.,\n",
    "$A-B=A+(-B)$.\n",
    "\n",
    "Euclidean distance between two points is the length of the path connecting them. In the plane, the distance between points $(x_1,y_1)$ and $(x_2,y_2)$ is given by the Pythagorean theorem:\n",
    "$$d=\\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}$$ \n",
    "\n",
    "\n",
    "\n",
    "There are other ways to measure distance. For example, the taxicab metric, also called the Manhattan distance, is the metric of the Euclidean plane defined by\n",
    "$g((x_1,y_1),(x_2,y_2))=|x_1-x_2|+|y_1-y_2|$\n",
    "\n",
    "for all points $P_1(x_1,y_1)$ and $P_2(x_2,y_2)$. This number is equal to the length of all paths connecting $P_1$ and $P_2$ along horizontal and vertical segments, without ever going back, like those described by a car moving in a lattice-like street pattern. \n",
    "\n",
    "\n",
    "The function that defines a distance between each pair of elements of a set is called metric. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c7a7304a941a4864f1c0ad2cce2d86b3",
     "grade": false,
     "grade_id": "math-euclid-q3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**c)** What is the (standard) *scalar product* of two vectors? How is it related to the length and angle between these vectors? Name some use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "e5c2f82430f4ba30219c7b0e9a43836d",
     "grade": true,
     "grade_id": "math-euclid-a3",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "The scalar (dot) product of two vectors $a$ and $b$ is a scalar quantity equal to the product of the magnitudes of the two vectors and the $cosine$ of the smallest angle between them:\n",
    "\n",
    " $$a·b=|a|⋅|b|\\cos{\\alpha}$$ \t\n",
    "\n",
    "\n",
    "where $\\alpha$ is the angle between the vectors and $|a|$, $|b|$ are the norms.\n",
    "\n",
    "Scalar (dot) product is a special case of a more general phenomenon known as an inner product. The inner product generalizes the dot product to abstract vector spaces over a field of scalars, being either the field of real numbers $\\mathbb{R}$ or the field of complex numbers $\\mathbb{C}$. It is usually denoted using angular brackets by $⟨ a , b ⟩$.\n",
    "\n",
    "\n",
    "The dot product can be used for computing the angle $α$ between two vectors a and b:\n",
    "\n",
    "$a⋅b=|a|⋅|b|⋅cos(α)$\n",
    "\n",
    "The sign of this expression depends only on the angle's cosine, therefore the dot product is\n",
    "\n",
    "     <0 if the angle is obtuse,\n",
    "     >0 if the angle is acute,\n",
    "     =0 if the a and b are orthogonal\n",
    "     \n",
    "Another application of the dot product, in combination with the cross product. With vectors a, b and c, which define a parallelepiped, its (signed) volume V can be computed:\n",
    "\n",
    "$$V=(a×b)⋅c$$\n",
    "\n",
    "This is a generalization of $|a×b|$ being the area of the parallelogram defined by $a$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "33cb9b299bd68d89dc69155cca58ae7b",
     "grade": false,
     "grade_id": "1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 1: Decision Trees [2 Points]\n",
    "Draw the decision trees for the following boolean functions. Either use pen and paper and bring the results to the feedback session or employ your ASCII artist within below. \n",
    "\n",
    "Note: $\\oplus := xor$, that means one of the operands has to be true, while the other one has to be false:\n",
    "\n",
    "$\\oplus$ | $B$ | $\\neg B$\n",
    "---------|-----|---------\n",
    "$A$      |  f  |    t\n",
    "$\\neg A$ |  t  |    f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "79a5db9b1525a51878c831a9d5fe5b35",
     "grade": false,
     "grade_id": "1a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### a) $\\neg A \\wedge B$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a3eacc13c6841e84a9e15f429370fa40",
     "grade": true,
     "grade_id": "1a_answer",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "                            -A-\n",
    "                    ---t---    ---f----\n",
    "                      No            -B-\n",
    "                            ---t---     ---f---    \n",
    "                              Yes         No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f500739a15568445866c4b555d7a6e36",
     "grade": false,
     "grade_id": "1b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### b) $A \\oplus B$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "081b38d16bbf73acc91330546dd4a73e",
     "grade": true,
     "grade_id": "1b_answer",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "                            ------A------\n",
    "                     ---t---             ---f----\n",
    "                       -B-                 -B-\n",
    "                ---t---   ---f---   ---t---   ---f---\n",
    "                  No        Yes        Yes       No\n",
    "                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c2e623bde4f83e6d64bc12abbdc8cbfc",
     "grade": false,
     "grade_id": "1c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### c) $A \\vee (B \\wedge C) \\vee (\\neg C \\wedge D)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "93daf8f9de9e7669bbd6994deb2f6f03",
     "grade": true,
     "grade_id": "1c_answer",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "                          ------A------\n",
    "                      ---t---       ---f---\n",
    "                        Yes       -----C-----\n",
    "                            ---t---          ---f---\n",
    "                              -B-              -D-\n",
    "                        ---t--- ---f---  ---t--- ---f---\n",
    "                          Yes      No      Yes     No\n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "74cd6378a22605b0e3e960e3ceb0d3ef",
     "grade": false,
     "grade_id": "1d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### d) $(A \\rightarrow (B \\wedge \\neg C)) \\vee (A \\wedge B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "13ffc3b5266121e14c1f8d82f95f2f5d",
     "grade": true,
     "grade_id": "1d_answer",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    " = $(\\neg A \\vee (B \\wedge \\neg C))\\vee (A \\wedge B)$\n",
    " \n",
    " = $ \\neg A \\vee (A \\wedge B) \\vee (B \\wedge \\neg C)$\n",
    " \n",
    " = $ \\neg A \\vee (A \\wedge B) $\n",
    " \n",
    "                           ------A------\n",
    "                      ---t---       ---f---\n",
    "                        -B-           Yes\n",
    "                ---t---     ---f---\n",
    "                  Yes          No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b60d7db6c515794ed215753fa117334f",
     "grade": false,
     "grade_id": "2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 2: Entropy and Information Gain [8 Points]\n",
    "\n",
    "In many machine learning applications it is crucial to determine which criterions are necessary for a good classification. Decision trees have those criterions close to the root, imposing an order from significant to less significant criterions. One way to select the most important criterion is to compare its information gain or its entropy to others. The following dataset is a hands-on example for this method. \n",
    "\n",
    "Consider the following attributes with their possible values:\n",
    "\n",
    "  * $raining = \\{yes, no\\}$\n",
    "  * $tired = \\{yes, no\\}$\n",
    "  * $late = \\{yes, no\\}$\n",
    "  * $distance = \\{short, medium, long\\}$\n",
    "\n",
    "And a training data set consisting of those attributes:\n",
    "\n",
    "| #  | raining | tired | late | distance | attend_party |\n",
    "|----|---------|-------|------|----------|--------------|\n",
    "| 1  | yes     | no    | no   | short    | **yes**      |\n",
    "| 2  | yes     | no    | yes  | medium   | **no**       |\n",
    "| 3  | no      | yes   | no   | long     | **no**       |\n",
    "| 4  | yes     | yes   | yes  | short    | **no**       |\n",
    "| 5  | yes     | no    | no   | short    | **yes**      |\n",
    "| 6  | no      | no    | no   | medium   | **yes**      |\n",
    "| 7  | no      | yes   | no   | long     | **no**       |\n",
    "| 8  | yes     | no    | yes  | short    | **no**       |\n",
    "| 9  | yes     | yes   | no   | short    | **yes**      |\n",
    "| 10 | no      | yes   | no   | medium   | **no**       |\n",
    "| 11 | no      | yes   | no   | long     | **no**       |\n",
    "| 12 | no      | yes   | yes  | short    | **no**       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d15839ac8d68a301dc4dc8449bc07a5b",
     "grade": false,
     "grade_id": "2a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### a)\n",
    "\n",
    "Build the root node of a decision tree from the training samples given in the table above by calculating the information gain for all four attributes (raining, tired, late, distance).\n",
    "\n",
    "$$\\operatorname{Gain}(S,A) = \\operatorname{Entropy}(S) - \\sum_{v \\in \\operatorname{Values}(A)} \\frac{|S_v|}{|S|}\\operatorname{Entropy}(S_v)$$\n",
    "\n",
    "$$\\operatorname{Entropy}(S) = -p_{\\oplus} log_{2} p_{\\oplus} - p_{\\ominus} log_{2} p_{\\ominus}$$\n",
    "\n",
    "$S$ is the set of all data samples. $S_v$ is the subset for which attribute $A$ has value $v$. An example for attribute **tired** with value $yes$ would be:\n",
    "$$|S_{yes}| = 7, S_{yes}:[1+, 6−]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "374b2672465f2f683716faee14ae8d12",
     "grade": true,
     "grade_id": "2a_answer",
     "locked": false,
     "points": 4,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "**Overall Entropy:**\n",
    "$$|S_{yes}|= 4$$\n",
    "\n",
    "$$|S_{no}| = 8$$\n",
    "\n",
    "$$Entropy(S) = -\\frac{4}{12} \\log_{2} \\frac{4}{12}-\\frac{8}{12} \\log_{2} \\frac{8}{12}$$\n",
    "\n",
    "$$= -\\frac{1}{3} \\log_{2} \\frac{1}{3}-\\frac{2}{3} \\log_{2} \\frac{2}{3} = 0.91829583405 \\approx 0.92$$\n",
    "\n",
    "**Raining:**\n",
    "$$|S_{yes}| = 6, S_{yes}:[3+, 3−]$$\n",
    "\n",
    "$$|S_{no}| = 6, S_{no}:[1+, 5-]$$\n",
    "\n",
    "$$Entropy(S_{yes}) = -\\frac{3}{6} \\log_{2} \\frac{3}{6}-\\frac{3}{6} \\log_{2} \\frac{3}{6}$$\n",
    "\n",
    "$$ = Entropy(S_{yes}) = -\\frac{1}{2} \\log_{2} \\frac{1}{2}-\\frac{1}{2} \\log_{2} \\frac{1}{2} = 1 $$\n",
    "\n",
    "$$Entropy(S_{no}) = -\\frac{1}{6} \\log_{2} \\frac{1}{6}-\\frac{5}{6} \\log_{2} \\frac{5}{6} = 0.65002242164 \\approx 0.65$$\n",
    "\n",
    "$$Gain(S,raining) \\approx 0.92 - (\\frac{6}{12} * 1 +\\frac{6}{12}*0.65)$$\n",
    "\n",
    "$$ \\approx 0.92 -(\\frac{1}{2} *1 + \\frac{1}{2}*0.65) = 0.095$$\n",
    "\n",
    "**Tired:**\n",
    "$$|S_{yes}| = 7, S_{yes}:[1+, 6−]$$\n",
    "\n",
    "$$|S_{no}| = 5, S_{no}:[3+, 2-]$$\n",
    "\n",
    "$$Entropy(S_{yes}) = -\\frac{1}{7} \\log_{2} \\frac{1}{7}-\\frac{6}{7} \\log_{2} \\frac{6}{7} = 0.59167277858 \\approx 0.59 $$\n",
    "\n",
    "$$Entropy(S_{no}) = -\\frac{3}{5} \\log_{2} \\frac{3}{5}-\\frac{2}{5} \\log_{2} \\frac{2}{5} = 0.97095059445 \\approx 0.97$$\n",
    "\n",
    "$$Gain(S,tired) \\approx 0.92 - (\\frac{7}{12} * 0.59 +\\frac{5}{12}*0.97) = 0.17166666666 \\approx 0.172$$\n",
    "\n",
    "**Late:**\n",
    "$$|S_{yes}| = 4, S_{yes}:[0+, 4−]$$\n",
    "\n",
    "$$|S_{no}| = 8, S_{no}:[4+, 4-]$$\n",
    "\n",
    "$$Entropy(S_{yes}) = -\\frac{0}{4} \\log_{2} \\frac{0}{4}-\\frac{4}{4} \\log_{2} \\frac{4}{4} = 0 $$\n",
    "\n",
    "$$Entropy(S_{no}) = -\\frac{4}{8} \\log_{2} \\frac{4}{8}-\\frac{4}{8} \\log_{2} \\frac{4}{8}$$\n",
    "$$ = -\\frac{1}{2} \\log_{2} \\frac{1}{2}-\\frac{1}{2} \\log_{2} \\frac{1}{2} = 1 $$\n",
    "\n",
    "$$Gain(S,late) \\approx 0.92 - (\\frac{4}{12} * 0 +\\frac{8}{12}*1)$$\n",
    "$$ \\approx 0.92 - (\\frac{1}{3} * 0 +\\frac{2}{3}*1)= 0.25333333333 \\approx 0.253$$\n",
    "\n",
    "**Distance:**\n",
    "$$|S_{short}| = 6, S_{short}:[3+, 3−]$$\n",
    "\n",
    "$$|S_{medium}| = 3, S_{medium}:[1+, 2-]$$\n",
    "\n",
    "$$|S_{long}| = 3, S_{long}:[0+, 3-]$$\n",
    "\n",
    "$$Entropy(S_{short}) = -\\frac{3}{6} \\log_{2} \\frac{3}{6}-\\frac{3}{6} \\log_{2} \\frac{3}{6}$$\n",
    "$$ = -\\frac{1}{2} \\log_{2} \\frac{1}{2}-\\frac{1}{2} \\log_{2} \\frac{1}{2} = 1$$\n",
    "\n",
    "$$Entropy(S_{medium}) = -\\frac{1}{3} \\log_{2} \\frac{1}{3}-\\frac{2}{3} \\log_{2} \\frac{2}{3} = 0.91829583405 \\approx 0.92$$\n",
    "\n",
    "$$Entropy(S_{long}) = -\\frac{0}{3} \\log_{2} \\frac{0}{3}-\\frac{3}{3} \\log_{2} \\frac{3}{3} = 0$$\n",
    "\n",
    "$$Gain(S,distance) \\approx 0.92 - (\\frac{6}{12} * 1 +\\frac{3}{12}*0.92+\\frac{3}{12}*0)$$\n",
    "$$ \\approx 0.92 - (\\frac{1}{2} * 1 +\\frac{1}{4}*0.92 + \\frac{1}{4}*0)= 0.19$$\n",
    "\n",
    "\n",
    "The root node should be **late** as it is the attribute with the greatest information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "564babda2996cead4eacceca3b3eeb64",
     "grade": false,
     "grade_id": "2b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### b)\n",
    "\n",
    "Perform the same calculation as in **a)** but use the gain ratio instead of the information gain. Does the result for the root node change?\n",
    "\n",
    "$$\\operatorname{GainRatio}(S,A) = \\frac{\\operatorname{Gain}(S,A)}{\\operatorname{SplitInformation}(S,A)}$$\n",
    "\n",
    "$$\\operatorname{SplitInformation}(S,A) = - \\sum_{v \\in \\operatorname{Values}(A)} \\frac{|S_v|}{|S|} \\log_{2} \\frac{|S_{v}|}{|S|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d0e7f2c03813216bf62ab5f0acab6a77",
     "grade": true,
     "grade_id": "2b_answer",
     "locked": false,
     "points": 4,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "**raining:**\n",
    "\n",
    "$$ SplitInformation(S,raining) = -(\\frac{6}{12} \\log_{2} \\frac{6}{12} + \\frac{6}{12} \\log_{2} \\frac{6}{12}) = 1$$\n",
    "$$ GainRatio(S,raining) = \\frac{0.095}{1} = 0.095$$\n",
    "\n",
    "**tired**\n",
    "$$ SplitInformation(S,tired) = -(\\frac{7}{12} \\log_{2} \\frac{7}{12} + \\frac{5}{12} \\log_{2} \\frac{5}{12}) = 0.98$$\n",
    "$$ GainRatio(S,tired) = \\frac{0.172}{0.98} = 0.17551020408 \\approx 0.176$$\n",
    "\n",
    "**late**\n",
    "$$ SplitInformation(S,late) = -(\\frac{4}{12} \\log_{2} \\frac{4}{12} + \\frac{8}{12} \\log_{2} \\frac{8}{12}) = 0.918$$\n",
    "$$ GainRatio(S,late) = \\frac{0.253}{0.918} = 0.27559912854 \\approx 0.276$$\n",
    "\n",
    "**distance**\n",
    "$$ SplitInformation(S,distance) = -(\\frac{6}{12} \\log_{2} \\frac{6}{12} + \\frac{3}{12} \\log_{2} \\frac{3}{12} + \\frac{3}{12} \\log_{2} \\frac{3}{12}) = 1.5$$\n",
    "$$ GainRatio(S,distance) = \\frac{0.19}{1.5} = 0.12666666666 \\approx 0.127$$\n",
    "\n",
    "The result for the root node doesn't change as the **late** attribute has still the highest gain ration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7d72fbdf1b31b6681ea8bb9d053979f6",
     "grade": false,
     "grade_id": "3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 3: ID3 algorithm [5 Points]\n",
    "\n",
    "Implement the following two functions in Python. Take a look at the `assert`s to see how the function should behave. An assert is a condition that your function is required to pass. Most of the conditions here are taken from the lecture slides (ML-03, Slide 12 & 13). Don't worry if you do not get all asserts to pass, just comment the failing ones out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a6c9e8564c72fc8093b7245bf3a9c652",
     "grade": false,
     "grade_id": "3a_info",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### a) Entropy\n",
    "\n",
    "$$\\operatorname{Entropy}(S) = - \\sum_{i=1...c} p_i log_2 p_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "ef2554da1e5f6b6a815a13e1c9f6912f",
     "grade": false,
     "grade_id": "3a_code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from math import log2\n",
    "def entropy(s):\n",
    "    \"\"\"\n",
    "    Calculate the entropy for a given target value set. \n",
    "    \n",
    "    Args:\n",
    "        s (list): Target classes for specific observations.\n",
    "        \n",
    "    Returns:\n",
    "        The entropy of s.\n",
    "    \"\"\"\n",
    "    #pi = how often is target class in list / length of list\n",
    "    return -sum([s.count(t)/ len(s) * log2(s.count(t)/len(s)) for t in set(s)])\n",
    "\n",
    "# See ML-03, Slide 12 & 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "120a1ed08a6d679643e859dd5ad08243",
     "grade": true,
     "grade_id": "3a_test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert entropy([1,1,1,0,0,0]) == 1.0\n",
    "assert round(entropy([1,1,1,1,0,0,0]), 3) == 0.985\n",
    "assert round(entropy([1,1,1,1,1,1,0]), 3) == 0.592\n",
    "assert round(entropy([1,1,1,1,1,1,0,0]), 3) == 0.811\n",
    "assert round(entropy([2,2,1,1,0,0]), 3) == 1.585\n",
    "assert round(entropy([2,2,2,1,0]), 3) == 1.371\n",
    "assert round(entropy([2,2,2,0,0]), 3) == 0.971"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5e8b62cf831972afa2ba4f73d8c70c53",
     "grade": false,
     "grade_id": "3b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### b)  Information Gain\n",
    "\n",
    "$$\\operatorname{Gain}(S,A) = \\operatorname{Entropy}(S) - \\sum_{v \\in \\operatorname{Values}(A)} \\frac{|S_v|}{|S|} \\operatorname{Entropy}(S_v)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "ebdc9b685ade2b882cd033ecc36c44ce",
     "grade": false,
     "grade_id": "3b_code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def gain(targets, attr_values):\n",
    "    \"\"\"\n",
    "    Calculates the expected reduction in entropy due to sorting on A.\n",
    "    \n",
    "    Args:\n",
    "        targets (list): Target classes for observations in attr_values.\n",
    "        attr_values (list): Values of each instance for the respective attribute.\n",
    "        \n",
    "    Returns:\n",
    "        The information gain of \n",
    "    \"\"\"\n",
    "    #weighted entropy after evaluation of attr_values \n",
    "    entropy_sorted = 0\n",
    "    #only use unique attribute values --> set\n",
    "    for value in set(attr_values):\n",
    "        #Sv = subset for which attribute A has value v \n",
    "        Sv = []\n",
    "        for (index, val) in enumerate(attr_values):\n",
    "            if val == value:\n",
    "                Sv.append(targets[index])     \n",
    "        #update entropy_sorted (sigma--> sum)\n",
    "        entropy_sorted = entropy_sorted + ((len(Sv)/len(targets))* entropy(Sv))  \n",
    "    return entropy(targets) - entropy_sorted\n",
    "\n",
    "# See ML-03, Slide 12 & 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "97b42b047c444b4d3c65aee8c1d44384",
     "grade": true,
     "grade_id": "3b_test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_S_ = [0,0,1,1,1,0,1,0,1,1,1,1,1,0]\n",
    "assert round(gain(assert_S_, [1,1,1,1,0,0,0,1,0,0,0,1,0,1]), 3) == 0.152\n",
    "assert round(gain(assert_S_, [0,1,0,0,0,1,1,0,0,0,1,1,0,1]), 3) == 0.048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2cd6e9dca6eb953989e6f7ef7a6ad37c",
     "grade": false,
     "grade_id": "3c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### c) ID3\n",
    "\n",
    "In the next two cells we have implemented the ID3 algorithm following the pseudocode from [Wikipedia](https://en.wikipedia.org/wiki/ID3_algorithm#Pseudocode) - it relies on your two functions from above, `entropy` and `gain`. Try to understand what the code does and replace `YOUR CODE HERE` with meaningful comments describing the respective parts of the code. Though its often annoying, being able to read other peoples code is one of the key skills (and obstacle) in software engineering. So give it a try! Otherwise you are of course welcome to write your own implementation.\n",
    "\n",
    "Below the algorithm's cell, it is applied to two data sets. Run those and discuss the differences. For which data set is the ID3 algorithm better suited and why? (Enter your answer in the cell below the code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "d4b63221966b12245e22fb99093c1e78",
     "grade": true,
     "grade_id": "3c_answer",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter, namedtuple\n",
    "\n",
    "\n",
    "class Node(namedtuple('Node', 'label children')):\n",
    "    \"\"\"\n",
    "    A small node representation with a pretty string representation.\n",
    "    \"\"\"\n",
    "    def __str__(self, level=0):\n",
    "        return_str ='{}{!s}\\n'.format(' ' * level * 4, self.label)\n",
    "        for child in self.children:\n",
    "            return_str += child.__str__(level + 1)\n",
    "        return return_str\n",
    "\n",
    "def id3(examples, attributes, target_attribute = None): \n",
    "    \"\"\"\n",
    "    Calculate a tree of Nodes (fields: label [string], children [list]) \n",
    "    using the ID3 algorithm found as pseudocode on Wikipedia.\n",
    "    \"\"\"\n",
    "    # create and return a single root node tree if all examples of same target and label node accordingly\n",
    "    if all(target == examples['targets'][0] for target in examples['targets']):\n",
    "        return Node('Result: {!s}'.format(examples['target_names'][examples['targets'][0]]), [])\n",
    "    \n",
    "    # If there is no predicting attribute just return the single node root tree with the label of the \n",
    "    # most common target attribute\n",
    "    if len(attributes) == 0:\n",
    "        attr = Counter(data_sample[target_attribute] for data_sample in examples['data']).most_common(1)\n",
    "        return Node('Attribute: {!s}, {!s} occurences'.format(examples['attributes'][target_attribute], attr), [])\n",
    "    \n",
    "    # Calculate the information gain and find the attribute that best classifies the example\n",
    "    gains = [gain(examples['targets'], [r[attribute] for r in examples['data']]) \n",
    "             for attribute in attributes]\n",
    "    max_gain_attribute = attributes[gains.index(max(gains))]\n",
    "    \n",
    "    # Create root node using previousy defined best classifier\n",
    "    root = Node('Attribute: {!s} (gain {!s})'.format(examples['attributes'][max_gain_attribute], \n",
    "                                                     round(max(gains), 4)), [])\n",
    "    \n",
    "    # For each possible value of the most classifying attribute (A)\n",
    "    for vi in set(data_sample[max_gain_attribute] for data_sample in examples['data']):\n",
    "        # add a new child node below the most classifying attribute\n",
    "        child = Node('Value: {!s}'.format(vi), [])\n",
    "        root.children.append(child)\n",
    "        \n",
    "        # examples_vi = subset of examples that have the value vi for A\n",
    "        vi_indices = [idx for idx, data_sample in enumerate(examples['data']) \n",
    "                          if data_sample[max_gain_attribute] == vi]\n",
    "        examples_vi = dict(examples)\n",
    "        examples_vi['data'] = [examples['data'][i] for i in vi_indices]\n",
    "        examples_vi['targets'] = [examples['targets'][i] for i in vi_indices]\n",
    "        \n",
    "        if examples_vi['data']:\n",
    "            # if examples_vi is not empty, add an ID3 subtree below new branch\n",
    "            child.children.append(\n",
    "                id3(examples_vi,\n",
    "                    [attribute_ for attribute_ in attributes if not attribute_ == max_gain_attribute],\n",
    "                    max_gain_attribute)\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            # if examples_vi is empty, add a lead node labelled with most common target value\n",
    "            attr = Counter(examples_vi['targets']).most_common(1)\n",
    "            label = 'Attribute: {!s}, {!s} occurences'.format(examples['attributes'][target_attribute], attr)\n",
    "            child.children.append(Node(label, []))\n",
    "\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the ID3 algorithm on the party data set which you already know from assignment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute: late (gain 0.2516)\n",
      "    Value: yes\n",
      "        Result: no\n",
      "    Value: no\n",
      "        Attribute: distance (gain 0.75)\n",
      "            Value: long\n",
      "                Result: no\n",
      "            Value: medium\n",
      "                Attribute: tired (gain 1.0)\n",
      "                    Value: yes\n",
      "                        Result: no\n",
      "                    Value: no\n",
      "                        Result: yes\n",
      "            Value: short\n",
      "                Result: yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('party.json', 'r') as party_file:\n",
    "    party = json.load(party_file)\n",
    "\n",
    "# Make sure our gain function handles the data set as expected.\n",
    "assert round(gain(party['targets'], [r[2] for r in party['data']]), 3) == 0.252\n",
    "\n",
    "# Apply ID3 algorithm\n",
    "tree_party = id3(party, list(range(len(party['attributes']))))\n",
    "\n",
    "print(tree_party)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs the ID3 algorithm on the famous iris flowser data set, which you will hear more about in assignment 4 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute: petal length (gain 1.4463)\n",
      "    Value: 6.7\n",
      "        Result: Iris-virginica\n",
      "    Value: 4.0\n",
      "        Result: Iris-versicolor\n",
      "    Value: 4.5\n",
      "        Attribute: sepal length (gain 0.5436)\n",
      "            Value: 6.4\n",
      "                Result: Iris-versicolor\n",
      "            Value: 4.9\n",
      "                Result: Iris-virginica\n",
      "            Value: 5.6\n",
      "                Result: Iris-versicolor\n",
      "            Value: 6.2\n",
      "                Result: Iris-versicolor\n",
      "            Value: 5.7\n",
      "                Result: Iris-versicolor\n",
      "            Value: 6.0\n",
      "                Result: Iris-versicolor\n",
      "            Value: 5.4\n",
      "                Result: Iris-versicolor\n",
      "    Value: 6.1\n",
      "        Result: Iris-virginica\n",
      "    Value: 1.3\n",
      "        Result: Iris-setosa\n",
      "    Value: 1.6\n",
      "        Result: Iris-setosa\n",
      "    Value: 1.1\n",
      "        Result: Iris-setosa\n",
      "    Value: 1.2\n",
      "        Result: Iris-setosa\n",
      "    Value: 6.3\n",
      "        Result: Iris-virginica\n",
      "    Value: 3.3\n",
      "        Result: Iris-versicolor\n",
      "    Value: 5.4\n",
      "        Result: Iris-virginica\n",
      "    Value: 5.2\n",
      "        Result: Iris-virginica\n",
      "    Value: 3.5\n",
      "        Result: Iris-versicolor\n",
      "    Value: 6.4\n",
      "        Result: Iris-virginica\n",
      "    Value: 4.9\n",
      "        Attribute: sepal width (gain 0.971)\n",
      "            Value: 3.1\n",
      "                Result: Iris-versicolor\n",
      "            Value: 2.8\n",
      "                Result: Iris-virginica\n",
      "            Value: 3.0\n",
      "                Result: Iris-virginica\n",
      "            Value: 2.5\n",
      "                Result: Iris-versicolor\n",
      "            Value: 2.7\n",
      "                Result: Iris-virginica\n",
      "    Value: 3.9\n",
      "        Result: Iris-versicolor\n",
      "    Value: 5.6\n",
      "        Result: Iris-virginica\n",
      "    Value: 3.6\n",
      "        Result: Iris-versicolor\n",
      "    Value: 4.3\n",
      "        Result: Iris-versicolor\n",
      "    Value: 5.7\n",
      "        Result: Iris-virginica\n",
      "    Value: 1.7\n",
      "        Result: Iris-setosa\n",
      "    Value: 1.0\n",
      "        Result: Iris-setosa\n",
      "    Value: 5.8\n",
      "        Result: Iris-virginica\n",
      "    Value: 5.1\n",
      "        Attribute: sepal length (gain 0.5436)\n",
      "            Value: 6.9\n",
      "                Result: Iris-virginica\n",
      "            Value: 5.9\n",
      "                Result: Iris-virginica\n",
      "            Value: 6.3\n",
      "                Result: Iris-virginica\n",
      "            Value: 6.0\n",
      "                Result: Iris-versicolor\n",
      "            Value: 5.8\n",
      "                Result: Iris-virginica\n",
      "            Value: 6.5\n",
      "                Result: Iris-virginica\n",
      "    Value: 4.6\n",
      "        Result: Iris-versicolor\n",
      "    Value: 5.9\n",
      "        Result: Iris-virginica\n",
      "    Value: 4.1\n",
      "        Result: Iris-versicolor\n",
      "    Value: 1.9\n",
      "        Result: Iris-setosa\n",
      "    Value: 1.5\n",
      "        Result: Iris-setosa\n",
      "    Value: 1.4\n",
      "        Result: Iris-setosa\n",
      "    Value: 3.8\n",
      "        Result: Iris-versicolor\n",
      "    Value: 6.6\n",
      "        Result: Iris-virginica\n",
      "    Value: 5.5\n",
      "        Result: Iris-virginica\n",
      "    Value: 4.8\n",
      "        Attribute: sepal length (gain 1.0)\n",
      "            Value: 6.2\n",
      "                Result: Iris-virginica\n",
      "            Value: 6.0\n",
      "                Result: Iris-virginica\n",
      "            Value: 6.8\n",
      "                Result: Iris-versicolor\n",
      "            Value: 5.9\n",
      "                Result: Iris-versicolor\n",
      "    Value: 4.7\n",
      "        Result: Iris-versicolor\n",
      "    Value: 6.9\n",
      "        Result: Iris-virginica\n",
      "    Value: 5.0\n",
      "        Attribute: sepal length (gain 0.8113)\n",
      "            Value: 6.7\n",
      "                Result: Iris-versicolor\n",
      "            Value: 6.3\n",
      "                Result: Iris-virginica\n",
      "            Value: 6.0\n",
      "                Result: Iris-virginica\n",
      "            Value: 5.7\n",
      "                Result: Iris-virginica\n",
      "    Value: 3.7\n",
      "        Result: Iris-versicolor\n",
      "    Value: 5.3\n",
      "        Result: Iris-virginica\n",
      "    Value: 3.0\n",
      "        Result: Iris-versicolor\n",
      "    Value: 4.4\n",
      "        Result: Iris-versicolor\n",
      "    Value: 6.0\n",
      "        Result: Iris-virginica\n",
      "    Value: 4.2\n",
      "        Result: Iris-versicolor\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('iris.json', 'r') as iris_file:\n",
    "    iris = json.load(iris_file)\n",
    "\n",
    "# Make sure our gain function handles the data set as expected.\n",
    "assert round(gain(iris['targets'], [r[2] for r in iris['data']]), 3) == 1.446\n",
    "\n",
    "# Apply ID3 algorithm\n",
    "tree_iris = id3(iris, list(range(len(iris['attributes']))))\n",
    "\n",
    "print(tree_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3a9118e68b062cd5d8433ef6e12d3437",
     "grade": true,
     "grade_id": "3d",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "*The ID3 is better suited for the **party** data as the iris data uses continuous variables and therefore each unique value is considered as a possible value for the attribute in the tree.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4: Decision Trees on Iris Flowers [5 Points]\n",
    "\n",
    "In this exercise we are going to examine and compare two decision trees that were generated from the iris flower data set ([Wikipedia](https://en.wikipedia.org/wiki/Iris_flower_data_set)) where three variations of the iris flower are quantified. The Iris data set is a classical example of a labeled dataset, i.e. every sample consists of two parts: features and labels. There are four features per sample in this data set (sepal length ($x_1$), sepal width ($x_2$), petal length ($x_2$) and petal width ($x_4$) in cm) and a corresponding label (Iris Setosa, Iris Versicolour, Iris Virginica). These samples are by nature **noisy**, no matter how carefully the measurement was taken - slight deviation from the actual length **cannot be avoided**. We want to learn how the features are related to the label so that we could (in the future) predict the label of a new sample automatically. One way to obtain such a `classifier` is to train a decision tree on the data.\n",
    "\n",
    "Here are two decisions tree generated by the data set. We will now take a closer look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree 1:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                      +\n",
    "                      |\n",
    "                      |\n",
    "                      |\n",
    "       x3 < 2.45      |     x3 >= 2.45\n",
    "   +------------------+------------------+\n",
    "   |                                     |\n",
    "   |                        x4 < 1.75    |     x4 >= 1.75\n",
    "   +                           +---------+---------+\n",
    "setosa                         |                   |\n",
    "                               |                   |\n",
    "                     x3 < 4.95 |   x3 >= 4.95      +\n",
    "                        +--------------+       virginica\n",
    "                        |              |\n",
    "                        |              |\n",
    "              x4 < 1.65 | x4 >= 1.65   +\n",
    "                 +------------+    virginica\n",
    "                 |            |\n",
    "                 |            |\n",
    "                 +            +\n",
    "            versicolor    virginica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree 2:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                      +\n",
    "                      |\n",
    "                      |\n",
    "                      |\n",
    "       x3 < 2.45      |     x3 >= 2.45\n",
    "   +------------------+------------------+\n",
    "   |                                     |\n",
    "   |                        x4 < 1.75    |     x4 >= 1.75\n",
    "   +                           +---------+---------+\n",
    "setosa                         |                   |\n",
    "                               |                   |\n",
    "                               +                   +\n",
    "                          versicolor           virginica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "What does it mean that the features $x1$ and $x2$ do not appear in the decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2eecbf907d099f392a93c6c70500246e",
     "grade": true,
     "grade_id": "4a",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "*Sepal length(x1) and sepal width(x2) do not seem to be important for classification.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "With which method from the lecture might the second tree have been generated from the first one? Explain the procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fa53969cbbf7c079a6ef4850d3cebda3",
     "grade": true,
     "grade_id": "4b",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "*The second tree might have been generated from the first one by using reduced error pruning which removes nodes in order to achieve better generalizations. It first evaluates the impact of pruning each node and then choses the node to be removed greedily on the basis of which node reduced error on the validation set the most.*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "After training the tree we can calculate the accuracy, i.e. the percentage of the training set that is classified correctly. Although the first tree was trained on the data set until no improvement of the accuracy was possible, its accuracy is *only* 98%. Explain why it is not 100 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e5382e843bc42ad7daa61898f5db7e95",
     "grade": true,
     "grade_id": "4c",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "*As mentioned above, the data is naturally noisy and therefore might contain some inconsistent data sets which can lead to misclassifications.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)\n",
    "\n",
    "Tree 2 only has a 96% accuracy on the training set. Why might this tree still be preferable over tree 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3c528275b25812856398b43be4465852",
     "grade": true,
     "grade_id": "4d",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "*Tree 2 will need less computation than tree 1 and will therefore be faster in classifying. Tree 1 also seems very specific to the data, which might indicate overfitting.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
