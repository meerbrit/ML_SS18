{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8f17c26ef5290d45b564a2185541d945",
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Osnabr√ºck University - Machine Learning (Summer Term 2018) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "277f219c2444bc880553e2854c23d23c",
     "grade": false,
     "grade_id": "h01",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true,
    "nbgrader": {
     "checksum": "99a16de3ab48ab9f2434ee6ee4fc335c",
     "grade": false,
     "grade_id": "h02",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Sunday, May 13, 2018**. If you need help (and Google and other resources were not enough), feel free to contact your groups designated tutor or whomever of us you run into first. Please upload your results to your group's studip folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a7f6ddc7ebfdba5ed4f5289fa7869988",
     "grade": false,
     "grade_id": "cell-0ceaa7378e4a713d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 0: Math recap (Derivatives in higher dimensions) [2 Bonus Points]\n",
    "\n",
    "This exercise is supposed to be very easy, does not give any points, and is voluntary. There will be a similar exercise on every sheet. It is intended to revise some basic mathematical notions that are assumed throughout this class and to allow you to check if you are comfortable with them. Usually you should have no problem to answer these questions offhand, but if you feel unsure, this is a good time to look them up again. You are always welcome to discuss questions with the tutors or in the practice session. Also, if you have a (math) topic you would like to recap, please let us know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6eb9238f1420f5fae66c47669e8d567d",
     "grade": false,
     "grade_id": "cell-106b918b6f9c6fea",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**a)** What is a partial derivative? What is a directional derivative? How are these computed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "36475d2d0f85070bc2f5c7617633e228",
     "grade": true,
     "grade_id": "cell-f80e2cfbc5dae96a",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "33165181f1f536046a89dea273b1dd3c",
     "grade": false,
     "grade_id": "cell-10c6f038150609e1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**b)** What is the gradient, the Jacobian matrix, and the Hessian matrix? How are they computed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "4ff3d7e15d56e4dbc54eaef465500577",
     "grade": true,
     "grade_id": "cell-c45db6ae30a5507a",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8ce944e185f8cdab12815c71da691493",
     "grade": false,
     "grade_id": "cell-7822385798587c45",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**c)** What is the chain rule (in calculus)? How does it look in the higher-dimensional case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "94696dd22018d912b2a84a73c0a13030",
     "grade": true,
     "grade_id": "cell-1a5e17baf68e02e1",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "checksum": "c22ba38b286bc0f20ba2aa172950677a",
     "grade": false,
     "grade_id": "ex1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 1: Curse of Dimensionality [6 Points]\n",
    "\n",
    "For the following exercise, be detailed in your answers and provide some examples. Think about keywords like: random vectors in high dimensional space, manifolds and Bertillonage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "checksum": "c82010f68e7434a489841f7cdedd8442",
     "grade": false,
     "grade_id": "ex1_a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**a)** What are the curse of dimensionality and its implication for pattern classification? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "checksum": "a81b14a51c8a9201f29c0f3fcd18e6c0",
     "grade": true,
     "grade_id": "ex1_a_solution",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "checksum": "c3ea2c6702a7f9c037c56ec86e64f9da",
     "grade": false,
     "grade_id": "ex1_b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**b)** Explain how this phenomenom could be used to one's advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "checksum": "2387cfdffc740aa306b5fb8ad7ac5312",
     "grade": true,
     "grade_id": "ex1_b_solution",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "checksum": "131868a391e6695a8f1273960e3dc171",
     "grade": false,
     "grade_id": "ex1_c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**c)** Explain in your own words the concepts of descriptive and intrinsic dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "checksum": "1d9a4f3cf1fcac05c64197baf4db13c1",
     "grade": true,
     "grade_id": "ex1_c_solution",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4812406918f98bb23619b761496c3759",
     "grade": false,
     "grade_id": "ex2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 2: Implement and Apply PCA [8 Points]\n",
    "\n",
    "In this assignment you will implement PCA from the ground up and apply it to the `cars` dataset (simplified from the JSE [2004 New Car and Truck Data](http://www.amstat.org/publications/jse/jse_data_archive.htm)). This dataset consists of measurements taken on 97 different cars. The eleven features measured are: Suggested retail price (USD), Price to dealer (USD), Engine size (liters), Number of engine cylinders, Engine horsepower, City gas mileage, Highway gas mileage, Weight (pounds), Wheelbase (inches), Length (inches) and Width (inches). \n",
    "\n",
    "We would like to visualize these high dimensional features to get a feeling for how the cars relate to each other so we need to find a subspace of dimension two or three into which we can project the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "325114529c4799bcf74afdf8403f3b99",
     "grade": true,
     "grade_id": "ex2_a_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: Load the cars dataset in cars.csv .\n",
    "# YOUR CODE HERE\n",
    "\n",
    "assert cars.shape == (97, 11), \"Shape is not (97, 11), was {}\".format(cars.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a70c090d0d56e70c7252fd74bfe83887",
     "grade": false,
     "grade_id": "ex2_b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "As a first step we need to normalize the data such that they have a zero mean and a unit standard deviation. Use the standard score for this:\n",
    "$$\\frac{X - \\mu}{\\sigma}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a168fa560a06737211cf8452e0096222",
     "grade": true,
     "grade_id": "ex2_b_solution",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: Normalize the data and store them in a variable called cars_norm.\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Alternatively one could use:\n",
    "# import sklearn.preprocessing\n",
    "# cars_norm = sklearn.preprocessing.scale(cars)\n",
    "\n",
    "assert cars_norm.shape == (97, 11), \"Shape is not (97, 11), was {}\".format(cars.shape)\n",
    "assert np.abs(np.sum(cars_norm)) < 1e-10, \"Absolute sum was {} but should be close to 0\".format(np.abs(np.sum(cars_norm)))\n",
    "assert np.abs(np.sum(cars_norm ** 2) / cars_norm.size - 1) < 1e-10, \"The data is not normalized, sum/N was {} not 1\".format(np.sum(cars_norm ** 2) / cars_norm.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2065e5ddf85831b70c0752661d6e5552",
     "grade": false,
     "grade_id": "ex2_c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "PCA finds a subspace that maximizes the variance by determining the eigenvectors of the covariance matrix. So we need to calculate the autocovariance matrix and afterwards the eigenvalues. When the data is normalized the autocovariance is calculated as\n",
    "$$C = X^T\\cdot X$$\n",
    "with $X$ being an $m \\times n$ matrix with $n$ features and $m$ samples.\n",
    "The entry $c_{i,j}$ in $C$ tells you how much feature $i$ correlates with feature $j$. \n",
    "(Note: sometimes the formula $C=X\\cdot X^T$ can be found, i.e., with rows and columns swapped. This depends on whether your put the individual datapoints as rows or columns in you matrix $X$. However, in the end you want to know how the individual features correlate, i.e., in our example you want a $11\\times11$-matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e9c3a8064dabec1cb95da24bcba6e073",
     "grade": true,
     "grade_id": "ex2_c_solution",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: Compute the autocovariance matrix and store it into autocovar\n",
    "# YOUR CODE HERE\n",
    "\n",
    "assert autocovar.shape == (11, 11)\n",
    "\n",
    "# TODO: Compute the eigenvalues und eigenvectors and store them into eigenval and eigenvec\n",
    "#       (Figure out a function to do this for you)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "assert eigenval.shape == (11,)\n",
    "assert eigenvec.shape == (11, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4e8e9da6d36f943a1cdce9a96208094e",
     "grade": false,
     "grade_id": "ex2_d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now you should have a matrix full of eigenvectors. We can now do two things: project the data down onto the two dimensional subspace to visualize it and we can also plot the two first principle component vectors as eleven two dimensional points to get a feeling for how the features are projected into the subspace. Execute the cells below and describe what you see. Is PCA a good method for this problem? Was it justifiable that we only considered the first two principle components? What kinds of cars are in the four quadrants of the first plot? (**put your answer in the cell below of this code cell**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project the data down into the two dimensional subspace\n",
    "proj = cars_norm @ eigenvec[:,0:2]\n",
    "\n",
    "# Plot projected data\n",
    "fig = plt.figure('Data projected onto first two Principal Components')\n",
    "fig.gca().set_xlim(-8, 8)\n",
    "fig.gca().set_ylim(-4, 7)\n",
    "plt.scatter(proj[:,0], proj[:,1])\n",
    "# Divide plot into quadrants\n",
    "plt.axhline(0, color='green')\n",
    "plt.axvline(0, color='green')\n",
    "# force drawing on 'run all'\n",
    "fig.canvas.draw()\n",
    "\n",
    "# Plot eigenvectors\n",
    "eig_fig = plt.figure('Eigenvector plot')\n",
    "plt.scatter(eigenvec[:,0], eigenvec[:,1])\n",
    "\n",
    "# add labels\n",
    "labels = ['Suggested retail price (USD)', 'Price to dealer (USD)', \n",
    "          'Engine size (liters)', 'Number of engine cylinders', \n",
    "          'Engine horsepower', 'City gas mileage' , \n",
    "          'Highway gas mileage', 'Weight (pounds)', \n",
    "          'Wheelbase (inches)', 'Length (inches)', 'Width (inches)']\n",
    "for label, x, y in zip(labels, eigenvec[:,0], eigenvec[:,1]):\n",
    "    plt.annotate(\n",
    "        label, xy = (x, y), xytext = (-20, 20),\n",
    "        textcoords = 'offset points', ha = 'left', va = 'bottom',\n",
    "        bbox = dict(boxstyle = 'round,pad=0.5', fc = 'blue', alpha = 0.5),\n",
    "        arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
    "# force drawing on 'run all'\n",
    "eig_fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e582879d8311a72a87b382a34ab12773",
     "grade": true,
     "grade_id": "ex2_d_solution",
     "locked": false,
     "points": 3,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true,
    "nbgrader": {
     "checksum": "c789f20eaf9c9d35f35ed957b6bcbabe",
     "grade": false,
     "grade_id": "ex3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Assignment 3: PCA [6 Points]\n",
    "\n",
    "In this exercise we investigate the statement from the lecture that PCA finds the subspace that captures most of the data variance. To be more precise, we show that the orthonormal projection onto an $m$-dimensional subspace that maximizes the variance of the projected data is defined by the principal components, i.e. by the $m$ eigenvectors of the autocorrelation matrix $C$ corresponding to the $m$ largest eigenvalues. We proceed in two steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "checksum": "0f2a172a236877e255e494e4a5d1b862",
     "grade": false,
     "grade_id": "ex3_a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### a)\n",
    "\n",
    "First consider a one dimensional subspace: determine a (unit) vector $\\vec{p}$, such that the variance of the data, when projected onto the subspace determined by that vector, is maximal.\n",
    "\n",
    "The autocorrelation matrix $C$ allows to compute the variance of the projected data as $\\vec{p}^{T}C\\vec{p}$. We want to maximize this expression. To avoid $\\|\\vec{p}\\|\\to\\infty$ we will only consider unit vectors, i.e. we constrain $\\vec{p}$ to be normalized: $\\vec{p}^T\\vec{p}=1$. Maximize the expression with this constraint (which can be done using a Lagrangian multiplier). Conclude that a suitable $\\vec{p}$ has to be an eigenvector of $C$ and describe which of the eigenvectors is optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "checksum": "5fac96e70057e82a477934dd7b030f98",
     "grade": true,
     "grade_id": "ex3_a_solution",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "checksum": "b337958cf77ef12fb8a20b29cf2a7540",
     "grade": false,
     "grade_id": "ex3_b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### b)\n",
    "\n",
    "Now proof the statement for the general case of an $m$-dimensional projection space.\n",
    "\n",
    "Use an inductive argument: assume the statement has been shown for the $(m-1)$-dimensional projection space, spanned by the $m-1$ (orthonormal) eigenvectors $\\vec{p}_1,\\ldots,\\vec{p}_{m-1}$ corresponding to the $(m-1)$ largest eigenvalues $\\lambda_1,\\ldots,\\lambda_{m-1}$. Now find a (unit) vector $\\vec{p}_m$, orthogonal to the existing vectors $\\vec{p}_1,\\ldots,\\vec{p}_{m-1}$, that maximizes the projected variance $\\vec{p}_m^TC\\vec{p}_m$. Proceed similar to case (a), but with additional Lagrangian multipliers to enforce the orthogonality constraint. Show that the new vector $\\vec{p}_m$ is an eigenvector of $C$. Finally show that the variance is maximized for the eigenvector corresponding to the $m$-th largest eigenvalue $\\lambda_m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "checksum": "24289a25e09f6f242db8c2793a892853",
     "grade": true,
     "grade_id": "ex3_b_solution",
     "locked": false,
     "points": 4,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
